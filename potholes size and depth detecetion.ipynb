{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d86c00",
   "metadata": {},
   "source": [
    "# Architecture explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37f79d",
   "metadata": {},
   "source": [
    "Please play the video. I have explained every thing in the video including all the script and folder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad930f",
   "metadata": {},
   "source": [
    "![title](re.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7e18a",
   "metadata": {},
   "source": [
    "# depth and size detcetion of the potholes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95b4ac",
   "metadata": {},
   "source": [
    "The main file of our work is that one. Is where we used the point cloud data of the road surface and projected it on the images to get the points inside each bounding box, which gave us the shape of the potholes. Using these techniques, we were able to estimate the depth and area of the potholes, and thus the magnitude of the potholes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b9597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'yolov7/'\n",
      "/home/anas/Desktop/HACTHON/yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946eae3",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914d3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "import open3d as o3\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ae12d",
   "metadata": {},
   "source": [
    "# Read points cloud/ intrinsic /extrinsic parameter of the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663aa2d",
   "metadata": {},
   "source": [
    "For both scene1, and 2 we have created the 3D point cloud using colmap open source software. Don't worry, the software provide a python API to automatise the process of creting those clouds. Colmap use structure from motion (SFM) approach to reconstruct the points cloud from the provided images. \n",
    "\n",
    "The two point clooud for both scenes are in folders S1 and S2. Make sure that they are exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a5660d91",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "Scene1 = PlyData.read('s1/PointcS1')\n",
    "Scene2 = PlyData.read('s2/PointcS2')\n",
    "s1_x = Scene1.elements[0].data['x']\n",
    "s1_y = Scene1.elements[0].data['y']\n",
    "s1_z = Scene1.elements[0].data['z']\n",
    "\n",
    "s2_x = Scene2.elements[0].data['x']\n",
    "s2_y = Scene2.elements[0].data['y']\n",
    "s2_z = Scene2.elements[0].data['z']\n",
    "\n",
    "sc1 = np.array([s1_x,s1_y,s1_z]).T\n",
    "sc2 = np.array([s2_x,s2_y,s2_z]).T\n",
    "\n",
    "sc1_p = o3.geometry.PointCloud()\n",
    "sc1_p.points = o3.utility.Vector3dVector(sc1)\n",
    "\n",
    "sc2_p = o3.geometry.PointCloud()\n",
    "sc2_p.points = o3.utility.Vector3dVector(sc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bc063f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.visualization.draw_geometries([sc2_p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c41096",
   "metadata": {},
   "source": [
    "Read extrinsic parameters (contain cameras position and its orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "438e2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('s2/images.txt','r') as line:\n",
    "    f=line.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89aace4",
   "metadata": {},
   "source": [
    "Read intrinsic parameters (contain the focal of the camera and the principal points also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1001f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('s2/cameras.txt','r') as line:\n",
    "    d=line.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ea0fe",
   "metadata": {},
   "source": [
    "Process intrinsic and extrinsic parameter. Convert them to numbers and pick only the position and orientation of the cameras + the focal + the principal points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "989e53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell extract the intinsic and extrinsic parameters.\n",
    "\n",
    "images = sorted(os.listdir('Challenge/png/Scene2'))\n",
    "path1 = 'anas1/exp/labels'\n",
    "path = 'Challenge/png/Scene2'\n",
    "\n",
    "def string_to_float_list(string):\n",
    "    float_numbers = [float(x) for x in re.findall(r'[+-]?\\d+\\.\\d+', string)]\n",
    "    return float_numbers\n",
    "\n",
    "#intrinsic cameras parameters\n",
    "intrinsic_list =[]\n",
    "for ele in d[3:]:\n",
    "    intrinsic_list.append(string_to_float_list(ele[16:])[0])\n",
    "\n",
    "# extract enrtinsic cameras parameteres\n",
    "extrisic_list =[]\n",
    "\n",
    "for image_name in images:\n",
    "    for ele in f:\n",
    "        if(image_name in ele):\n",
    "            extrisic_list.append(string_to_float_list(ele[1:-9]))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1780d9",
   "metadata": {},
   "source": [
    "We present here some function that will serve us in the process of finding the depth of the potholes. I want to mention here on important thing !!!!!!.\n",
    "\n",
    "the results of calculating depth, size, bounding box, evnt the calculated points cloud are all in pixel unit. We need to convert it to (cm) to give senses to results output. One of the approach is to calculate the ratio which we need to know the physical size of the object being imaged and the number of pixels that represent that object in the image.\n",
    "\n",
    "For example, if you know that an object in the real world is 10 cm long and it's represented by 100 pixels in the image, you can convert pixels to centimeters using the following formula:\n",
    "\n",
    "ratio = real_world_size_in_cm / pixels_in_image\n",
    "\n",
    "so we need to multiply the depth ans size and ... with the ratio. \n",
    "\n",
    "Here we have approximate manually one pothole (just by eye 27 cm) and we use its bounding box to find pixel number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d86b9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 27\n",
    "pixel = 130\n",
    "ratio = 27/130\n",
    "\n",
    "# Draw the bounding box on the image and show the depth and size after calculation.\n",
    "def draw_bounding_box_with_size_and_depth(img, x_min, y_min, x_max, y_max, depth, proj):\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    # Calculate the size and depth of the bounding box\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    size = width * height * ratio\n",
    "    size = 'size:'+str(round(size,1))+'cm2'\n",
    "    depth = 'depth:'+str(depth)+'cm'\n",
    "    # Add text on the bounding box\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img, size, (x_min + 5, y_max + 20), font, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(img, str(depth), (x_min + 5, y_max + 40), font, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    for point in proj:\n",
    "        x, y = point[0], point[1]\n",
    "        cv2.circle(frame, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "    return img\n",
    "\n",
    "# bounding box convert from x,y,width,high to two x,y and x1,y1 for cv2 drawing.\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y =  np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "# This function calculate the inverse of the homogenous matrix\n",
    "def invert_ht(ht):\n",
    "    ht = np.tile(ht, [1, 1, 1])\n",
    "    iht = np.tile(np.identity(4), [ht.shape[0], 1, 1])\n",
    "    iht[..., :3, :3] = ht[..., :3, :3].transpose(0, 2, 1)\n",
    "    iht[..., :3, [3]] = -np.matmul(iht[..., :3, :3], ht[..., :3, [3]])\n",
    "    return iht.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8314d",
   "metadata": {},
   "source": [
    "Aligning the points cloud with the cooresponding image based on the extrinsic cameras parameter.\n",
    "Example aligning the points cloud with image '0001.png' is to use it is  extrinsic parameter\n",
    "which contain its position and orientation by a homogunous transformation matrix which it help to transform \n",
    "any points cloud to the willing distance and to the willing direction just by simple multiplication.\n",
    "I left you here a link to read more http://motion.cs.illinois.edu/RoboticSystems/CoordinateTransformations.html\n",
    "The aim by this aliginement is to give sense to our projection because we can not project the points clouds\n",
    "on the image without being both in same reference coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4cc2265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Homogenous_transformation(extrinsic, cloud1):\n",
    "    cloud = copy.deepcopy(cloud1)\n",
    "    transformation = np.identity(4)\n",
    "    # Transform the point cloud using the extrinsic parameters\n",
    "    R = cloud.get_rotation_matrix_from_quaternion(extrinsic[:4])\n",
    "    transformation[:3,:3] = R\n",
    "    transformation[:3,3] = [extrinsic[4], extrinsic[5], extrinsic[6]]\n",
    "    cloud = cloud.transform(transformation)\n",
    "    cloud = np.array(cloud.points)\n",
    "    \n",
    "    return cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c5b6d",
   "metadata": {},
   "source": [
    "This a backward function which back from the image reference system to the body reference system. why using this. \n",
    "because to calculate the depth of the potholes we need to considere the whole reference system which make sence\n",
    "comparing if we compute it just for the image reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "839c9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inverse_Homogenous_transformation(extrinsic, cloud1):\n",
    "    cloud2 = o3.geometry.PointCloud()\n",
    "    cloud2.points = o3.utility.Vector3dVector(cloud1)\n",
    "    cloud = copy.deepcopy(cloud2)\n",
    "    transformation = np.identity(4)\n",
    "    # Transform the point cloud using the extrinsic parameters\n",
    "    R = cloud.get_rotation_matrix_from_quaternion(extrinsic[:4])\n",
    "    transformation[:3,:3] = R\n",
    "    transformation[:3,3] = [extrinsic[4], extrinsic[5], extrinsic[6]]\n",
    "    inv_T = invert_ht(transformation)\n",
    "    cloud = cloud.transform(inv_T)\n",
    "    cloud = np.array(cloud.points)\n",
    "    \n",
    "    return cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7271b",
   "metadata": {},
   "source": [
    "After alignement, the projection of the points cloud on the image and also provide the 3D points included in each bounding box [IMPORTANT] \n",
    "because z is the depth of the so we can deduce the of the potoles by [IMPORTANT] calculating the min of z of all the points inside the bounding box [IMPORTANT] \n",
    "\n",
    "The focal and principal point extracted from the the results colmap execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc4cd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def projection(intrinsic_list, extrinsic, image_cloud, bounding_box):\n",
    "    image_clouds = []\n",
    "    depth = []\n",
    "    # Get the intrinsic parameters of the camera\n",
    "    focal_length = intrinsic_list\n",
    "    principal_point = (960, 540)\n",
    "\n",
    "    # Project the point cloud onto the image plane\n",
    "    image_coordinates = image_cloud[:, :2] / image_cloud[:, 2:]\n",
    "    image_coordinates[:, 0] = (image_coordinates[:, 0] * focal_length) + principal_point[0]\n",
    "    image_coordinates[:, 1] = (image_coordinates[:, 1] * focal_length) + principal_point[1]\n",
    "    # Extract the points inside the bounding box\n",
    "    for bbox in bounding_box:\n",
    "        x_min, y_min, x_max,  y_max = bbox\n",
    "        inliers = np.where((image_coordinates[:, 0] > x_min) & \n",
    "                           (image_coordinates[:, 0] < x_max) & \n",
    "                           (image_coordinates[:, 1] > y_min) & \n",
    "                           (image_coordinates[:, 1] < y_max))\n",
    "        image_cloud1 = image_coordinates[inliers]\n",
    "        image_clouds.append(image_cloud1)\n",
    "        \n",
    "        dep = inverse_Homogenous_transformation(extrinsic, image_cloud[inliers])\n",
    "    \n",
    "        if(len(dep[:,2])>0):\n",
    "            depth.append(round(np.min(dep[:,2])*ratio-1,1))\n",
    "        else:\n",
    "            depth.append(0)\n",
    "       \n",
    "    return image_clouds, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2d982",
   "metadata": {},
   "source": [
    "This is the main loop of this project which do the following\n",
    "\n",
    "1- Align the point cloud with each image \n",
    "\n",
    "2- project the point cloud on the image\n",
    "\n",
    "3- pick the points inside the bounding box (if exist look to challenges bellow)\n",
    "\n",
    "4- the points inside each bounding box should represent the shape of the pothole so we can calculate the depth of the pothole by calculating the min of all the points which is logic to be the depth. For the size we considere it as a surface of a rectangle x*y which is in cm2\n",
    "\n",
    "5- refer to the folder projectio to see the results of projection and also the depth and size value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d2d91ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind, image_name in enumerate(images):\n",
    "    ext = extrisic_list[ind]\n",
    "    ixt = intrinsic_list[ind]\n",
    "    image_path = os.path.join(path, image_name)\n",
    "    frame = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = frame.shape\n",
    "    bounding_boxes = []\n",
    "    \n",
    "    if os.path.exists(os.path.join(path1,image_name[:-4]+'.txt')):\n",
    "        with open(os.path.join(path1,image_name[:-4]+'.txt'), 'r') as file:\n",
    "            for line in file:\n",
    "                class_name, x, y, w, h = line.strip().split()\n",
    "                bounding_boxes.append([float(x)*img_width, float(y)*img_height, float(w)*img_width, float(h)*img_height])\n",
    "    if(len(bounding_boxes)>0):\n",
    "        bounding_boxes = xywh2xyxy(np.array(bounding_boxes))\n",
    "        image_clouds = Homogenous_transformation(ext, sc2_p)\n",
    "        proj, depth = projection(ixt, ext, image_clouds, bounding_boxes)\n",
    "        for j, bbox in enumerate(bounding_boxes):\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            draw_bounding_box_with_size_and_depth(frame, int(xmin), int(ymin), int(xmax), int(ymax), depth[j] , proj[j] )\n",
    "        cv2.imwrite(\"projection/{}.jpg\".format(ind+1), frame)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff6478",
   "metadata": {},
   "source": [
    "The severity now can be measured by the depth and size of the potholes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea24b2",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4db8c",
   "metadata": {},
   "source": [
    "1 - knowing the ratio calculated above\n",
    "\n",
    "2 - the quality of point cloud to improve\n",
    "\n",
    "3 - some bounding box contain a few amount of points which do not give precision\n",
    "\n",
    "4 - need a system to track each bounding box to optimze the calculation time because if a pothole is already calculated, there is no matter to recalculate it in next...\n",
    "\n",
    "5 - Handling overlapping and inclusion between bounding box "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067eaf8",
   "metadata": {},
   "source": [
    "# Thanks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
